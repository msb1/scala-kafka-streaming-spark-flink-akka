package Streams

import java.io.{ByteArrayOutputStream, File}
import java.util.Properties

import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper
import libsvm.{svm, svm_node}
import ml.dmlc.xgboost4j.scala.XGBoost
import org.apache.kafka.streams.scala._
import org.apache.kafka.streams.scala.kstream._
import org.apache.kafka.streams.{StreamsConfig, Topology}
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
import utils.JSONSerde

import scala.collection.immutable.HashMap


object StreamML {

  // case classes for record deserialization
  case class EpdData(uuid: String, CurrentTime: String, Topic: String, Categories: HashMap[String, Int], Sensors: HashMap[String, Double], Result: Int)
  // case class for preprocessed records
  case class PreProcessed(key: String, uuid: String, recordTime: String, features: Array[Float])
  // case class for ML results
  case class Result(key: String, uuid: String, recordTime: String, prediction: Int)

  // Kafka properties and config
  val props = new Properties()
  props.put(StreamsConfig.APPLICATION_ID_CONFIG, "barnwaldo") // group.Id for Kafka Streams
  props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.21.10:9092")
  props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String.getClass)
  props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String.getClass)
  props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000.asInstanceOf[Object])

  val consumerTopic = "str01"
  val producerTopic = "epd01"
  val epdSerde = new JSONSerde[EpdData]

  // Jackson Json to/from type
  val objectMapper = new ObjectMapper() with ScalaObjectMapper
  objectMapper.registerModule(DefaultScalaModule)

  def jsonToType[T](json: String)(implicit m: Manifest[T]): T = {
    objectMapper.readValue[T](json)
  }

  // upload logistic regression DL4J model
  val lrFile = new File("lr-dl4j-model.zip")
  val lrModel = MultiLayerNetwork.load(lrFile, false)
  // upload gradient boosted trees and random forest XGBoost models
  val gbtModel = XGBoost.loadModel("xgb.model")
  val rfModel = XGBoost.loadModel("rf.model")
  // upload SVC model
  val svmModel = svm.svm_load_model("svm.epd.model")

  def createTopology(): Topology = {

    val builder: StreamsBuilder = new StreamsBuilder

    // KStream for chunks of characters - check Trie with each character read
    val epdStream: Array[KStream[String, PreProcessed]] = builder
      .stream[String, EpdData](consumerTopic)(Consumed.`with`(Serdes.String, epdSerde))
      .map((key, value) => (key, PreProcessed(key, value.uuid, value.CurrentTime, StreamHelper.createDataRecord(value))))
        .branch(
          (key, _) => key == "LR",
          (key, _) => key == "SVC",
          (key, _) => key == "RF",
          (key, _) => key == "GBT",
        )

    // stream to logistic regression
    epdStream(0)
      .map((key, value) => {
        val ind = StreamHelper.createINDArray(value.features)
        val probs = lrModel.output(ind)
        var prediction: Int = 0
        if (probs.getDouble(1L) > probs.getDouble(0L)) {
          prediction = 1
        }
        // println(s"Key: ${value.key} -- uuid: ${value.uuid} -- recordTime: ${value.recordTime} -- Prediction: $prediction")
        val out = new ByteArrayOutputStream()
        objectMapper.writeValue(out, Result(key, value.uuid, value.recordTime, prediction))
        (key, out.toString)
      }).to(producerTopic)(Produced.`with`(Serdes.String, Serdes.String))

    // stream to support vector machine
    epdStream(1)
      .map((key, value) => {
        val nodes: Array[svm_node] = Array.fill(StreamHelper.numFeature)(new svm_node())
        for (col <- 0 until StreamHelper.numFeature) {
          nodes(col).index = col
          nodes(col).value = value.features(col)
        }
        val prediction = Math.round(svm.svm_predict(svmModel, nodes)).toInt
        val out = new ByteArrayOutputStream()
        objectMapper.writeValue(out, Result(key, value.uuid, value.recordTime, prediction))
        (key, out.toString)
      }).to(producerTopic)(Produced.`with`(Serdes.String, Serdes.String))

    // stream to Random Forest
    epdStream(2)
      .map((key, value) => {
        val dmatrix = StreamHelper.createDMatrix(value.features)
        val output = rfModel.predict(dmatrix)
        val prediction = Math.round(output(0)(0))
        val out = new ByteArrayOutputStream()
        objectMapper.writeValue(out, Result(key, value.uuid, value.recordTime, prediction))
        (key, out.toString)
      }).to(producerTopic)(Produced.`with`(Serdes.String, Serdes.String))

    // stream to Gradient Boosted Tree
    epdStream(3)
      .map((key, value) => {
        val dmatrix = StreamHelper.createDMatrix(value.features)
        val output = gbtModel.predict(dmatrix)
        val prediction = Math.round(output(0)(0))
        val out = new ByteArrayOutputStream()
        objectMapper.writeValue(out, Result(key, value.uuid, value.recordTime, prediction))
        (key, out.toString)
      }).to(producerTopic)(Produced.`with`(Serdes.String, Serdes.String))

    builder.build()
  }

}
